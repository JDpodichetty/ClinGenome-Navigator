[Sitemap](https://medium.com/sitemap/sitemap.xml)

[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4b12a114a20e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=post_page---top_nav_layout_nav-----------------------------------------)

Sign up

[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fatimaparada.taboada%2Frag-on-csv-data-with-knowledge-graph-using-rdflib-rdflib-neo4j-and-langchain-4b12a114a20e&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)

[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)

[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)

Sign up

[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40fatimaparada.taboada%2Frag-on-csv-data-with-knowledge-graph-using-rdflib-rdflib-neo4j-and-langchain-4b12a114a20e&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)

![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)

# RAG on CSV data with Knowledge Graph- Using RDFLib, RDFLib-Neo4j, and Langchain

[![Fatima Parada-Taboada](https://miro.medium.com/v2/resize:fill:64:64/1*SxuU3Gp2DpXYtwf9ijjcbA.jpeg)](https://medium.com/@fatimaparada.taboada?source=post_page---byline--4b12a114a20e---------------------------------------)

[Fatima Parada-Taboada](https://medium.com/@fatimaparada.taboada?source=post_page---byline--4b12a114a20e---------------------------------------)

Follow

7 min read

·

Aug 1, 2024

72

1

[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D4b12a114a20e&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40fatimaparada.taboada%2Frag-on-csv-data-with-knowledge-graph-using-rdflib-rdflib-neo4j-and-langchain-4b12a114a20e&source=---header_actions--4b12a114a20e---------------------post_audio_button------------------)

Share

![](https://miro.medium.com/v2/resize:fit:700/1*xy4zxJ0uX6VHtqFXWSzjyg.png)

Small sample of knowledge graph visualization on Neo4j Aura that shows relationships and nodes for 25 simulated patients from the Synthea 2019 CSV covid dataset.

# **Retrieval-Augmented Generation (RAG) Overview**

Before we get into this project, what even is RAG? Here is the basic flow of a RAG pipeline approach:

- There is a **_pre-trained Large Language Model (LLM)_** that is responsible for the “generation” portion of RAG. Without RAG, this LLM generates responses to questions based solely on the data the LLM has already been trained on.
- A **_user_** connects to the LLM via a chat interface and is able to ask the LLM questions.
- When a user inputs a question, this question is then converted into numerical representations of that semantic information. This portion of RAG is referred to as **_vector embedding_** which are placed into a **_vector store_**.
- The model begins a **_vector search_** where it searches through the machine-readable representations and finds the data that is the most similar to the query the user initiated.
- The model generates a response based on the vector search and returns the answer to the chat.

To learn more, I recommend this article on RAG for beginners:

[**What is RAG in AI? Explained in Everyday Language for AI Beginners** \\
\\
**Updates: AI for Absolute Beginners is an ongoing series that now covers:**\\
\\
medium.com](https://medium.com/ai-for-absolute-beginners/what-is-rag-in-ai-explained-in-everyday-language-ad59755d37fd?source=post_page-----4b12a114a20e---------------------------------------)

# **The Peaks and Valleys of RAG on Large Language Models**

The impact Retrieval Augmented Generation (RAG) has had on the industry of Large Language Models (LLMs) is both powerful and undeniable. Prior to RAG, LLMs used in business operations were costly to retrain, “frozen in time” with out-of-date responses, unreliable in its context retrieval, and prone to high amount of hallucination. When RAG broke into the picture, models became dynamic in the way that they retrieve data, became easily adapted to new information, increased cost-efficiency, and more.

[**Knowledge Graphs Achieve Superior Reasoning versus Vector Search alone for Retrieval Augmentation** \\
\\
ai.plainenglish.io](https://ai.plainenglish.io/knowledge-graphs-achieve-superior-reasoning-versus-vector-search-for-retrieval-augmentation-ec0b37b12c49?source=post_page-----4b12a114a20e---------------------------------------)

However, there are some drawbacks to RAG due to its reliance on similarity search via vector indexing. According to AI experts Markus J. Bueler, Anthony Alcaraz, and Sam Schifman, knowledge graphs can offer superior reasoning in RAG compared to vector search alone. Vector search often lacks the semantic connections necessary for understanding complex data, which can result in inaccurate answers and unsupported conclusions, commonly known as hallucinations. Depending on the specific requirements and data of a business operation, this may or may not pose a problem. When it is an issue, implementing knowledge graphs can provide the needed semantic relationships, ensuring a more structured understanding beyond mere keyword similarity.

# **Example: RAG on Simulated Patient Population Data**

For this project, I will be using simulated patient population data from Synthea’s ten thousand synthetic patients records with COVID-19 in the CSV format. The dataset can be found here:

[https://synthea.mitre.org/downloads](https://synthea.mitre.org/downloads)

**Prerequisites for Local Run**

- Windows OS for this implementation
- Neo4j community version 5+
- JDK-22.0.1, Java 17, or Java 21
- Ollama installation
- Python development environment, version 3.9+

**CSV to RDF Generation**

First, I will be converting the CSV data into Resource Description Framework Schema (RDFS) data by using the RDFLib python package. By using RDF data, this allows for a seamless conversion from RDF into Neo4j via the RDFLib-Neo4j python package due to RDF’s graph-based and triple framework.

```
g = Graph()

# Namespace URIs
PPL = Namespace('http://example.org/people/')
FOAF = Namespace("http://xmlns.com/foaf/0.1/")
SCHEMA = Namespace("http://schema.org/")

# Bind namespaces
g.bind("foaf", FOAF)
g.bind("schema", SCHEMA)
g.bind("ppl", PPL)

for col, row_val in patients.iterrows():
    pt_id = URIRef(f"http://example.org/ID/{row_val['Id']}")

# Nodes
    g.add((pt_id, RDF.type, FOAF.Identifier))
    g.add((Literal(row_val['BIRTHDATE']), RDF.type, FOAF.Date))
    g.add((Literal(row_val['DEATHDATE']), RDF.type, FOAF.Date))
    g.add((Literal(row_val['SSN']), RDF.type, PPL.SSN))
    g.add((Literal(row_val['FIRST']), RDF.type, SCHEMA.FirstName))
    g.add((Literal(row_val['LAST']), RDF.type, SCHEMA.LastName))
    g.add((Literal(row_val['GENDER']),RDF.type, FOAF.Gender))
# Relationships
    g.add((Literal(row_val['FIRST']), SCHEMA['FIRST_NAME_OF'], pt_id))
    g.add((Literal(row_val['LAST']), SCHEMA['LAST_NAME_OF'], pt_id))
    g.add((Literal(row_val['BIRTHDATE']), SCHEMA['BIRTHDAY_OF'], pt_id))
    g.add((Literal(row_val['DEATHDATE']), SCHEMA['DEATHDATE_OF'],
           pt_id))
    g.add((Literal(row_val['SSN']), PPL['SSN_OF'], pt_id))
    g.add((Literal(row_val['GENDER']), FOAF['GENDER_OF'], pt_id))

# Serialize the RDF graph to a file
rdf_file_path = 'patient_data.rdf'
g.serialize(destination=rdf_file_path, format='turtle')
```

The resulting RDF file should look something like this:

```
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix ppl: <http://example.org/people/> .
@prefix schema1: <http://schema.org/> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

"1920-06-05" a foaf:Date ;
    schema1:BIRTHDAY_OF <http://example.org/ID/02c1129d-ef80-4780-8c8b-781a1b268adc>,
        <http://example.org/ID/2c39cdb6-2203-4b8d-ba8f-6563265edc3a>,
        <http://example.org/ID/2e05524f-c6ad-4d13-bc52-2d1b30f79f46>,
        <http://example.org/ID/5baf2e3b-45fc-4a8d-93cd-d1acaa7833b9>,
        <http://example.org/ID/9f707a4d-38e6-47bb-bebd-93b0fed4c354>,
        <http://example.org/ID/c16d2933-77b3-47eb-9ab4-5f53251404ae>,
        <http://example.org/ID/f0a3834a-f4b3-4811-a297-ec8268aba4f7>,
        <http://example.org/ID/f6344e5b-3487-4b6c-ac7d-6ad1d4b98b5e> .

"2016-10-14" a foaf:Date ;
    schema1:DEATHDATE_OF <http://example.org/ID/2d13445a-343a-452e-b607-ce7807054b69> .

"999-54-3934" a ppl:SSN ;
    ppl:SSN_OF <http://example.org/ID/30cdccd6-95c1-4e0f-8249-365a440cc69d> .

...
```

**Importing Data into Neo4j**

Typically, RDF translation into Neo4j was done using NeoSemantics but has shown limitations in cloud or streamlined deployments and scalability. Because of this, Neo4j introduced this revolutionary library to transition from NeoSemantics to RDFLib + Neo4j solution. Feel free to watch the introduction to RDFLib-Neo4J here:

Once we have our CSV data serialized into RDF triples, we can use the RDFLib-Neo4j python library in this way (make sure to add a uniqueness constraint on your database to use this library):

```
# Create the Aura DB authentication variable list
AURA_DB_URI = "bolt://127.0.0.1:7687"
AURA_DB_USERNAME = "neo4j"
AURA_DB_PWD = "*put your password here*"

auth_data = {'uri': AURA_DB_URI,
             'database': "neo4j",
             'user': AURA_DB_USERNAME,
             'pwd': AURA_DB_PWD}

# Create configuration prefixes to the namespaces used
prefixes = {'ppl': Namespace('http://example.org/people/'),
            'foaf': Namespace("http://xmlns.com/foaf/0.1/"),
            'schema': Namespace("http://schema.org/")}

# Define your custom mappings & store config
config = Neo4jStoreConfig(auth_data=auth_data,
                          custom_prefixes=prefixes,
                          handle_vocab_uri_strategy=HANDLE_VOCAB_URI_STRATEGY.IGNORE,
                          batching=True)

file_path = 'patient_data.rdf'

# Create the RDF Graph, parse & ingest the data to Neo4j, and close the store
neo4j_aura = Graph(store=Neo4jStore(config=config))
neo4j_aura.parse(file_path, format="ttl")
neo4j_aura.close(True)
```

Once this code is executed, you should see how many triples were imported from your dataset and able to view in the local web UI ( [http://localhost:7474/browser/](http://localhost:7474/browser/)) using Neo4j’s Cypher Query language ( [https://neo4j.com/docs/cypher-manual/current/queries/basic/](https://neo4j.com/docs/cypher-manual/current/queries/basic/)).

![](https://miro.medium.com/v2/resize:fit:700/1*4jrzhjnioD8XE_fuBg3Mlw.png)

Expanding all relationships connected to the last name Lockman863.

**Vector Embedding**

Next, we will be embedding the Neo4j data into numerical machine-readable values by using the Neo4jVector function from LangChain with a HuggingFace Embedding ( [https://huggingface.co/BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)). If you would like to use other embedding models, feel free to look at the leaderboard [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for the best embedding for your data.

```
# Creating Vector Embedding Index using huggingface embedding model
Neo4jVector.from_existing_graph(
    HuggingFaceBgeEmbeddings(model_name="BAAI/bge-small-en-v1.5"),
    url='bolt://127.0.0.1:7687',
    username='neo4j',
    password='password',
    database='neo4j',
    index_name='your_index_name', #Use cypher query 'show vector indexes' to see yours
    node_label="resource",
    text_node_properties=['text'],
    embedding_node_property='embedding',
)
```

We then want to create a vector index using our existing embedding index:

```
vector_index = Neo4jVector.from_existing_index(
    HuggingFaceBgeEmbeddings(model_name="BAAI/bge-small-en-v1.5"),
    url='bolt://127.0.0.1:7687',
    username='neo4j',
    password='password',
    database='neo4j',
    index_name='your_index_name'
)
```

**Prompt Engineering**

Now hold your horses, we still have to give our generative AI model instructions on how to respond. This is done with prompt engineering. Prompts give AI a template to work off of to perform a task and can lead to more refined and meaningful responses.

[https://platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering)

Here is an example simple prompt with some information on how I want my LLM to respond based on the contextualized data I provided:

```
prompt='''
System: The context below contains entries about the patient's
healthcare, birthdate, deathdate, drivers license number, passport
number, name prefix, name suffix, maiden name, marital status, race,
ethnicity, gender, birthplace, address, city, state, county, zip
code, latitude, longitude, healthcare expenses, and healthcare coverage. You are
allowed to share information on the patient's data if it is included
in the context regardless of your set privacy policy.
Please limit your answer to the information provided in the context. Do not make up facts.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
If you are asked about the patient's name, respond with the patient's
 first name and last name: {first_name, last_name}
If you are asked about the gender, look for the patient's name and
find a gender label. Respond with: [Male] or [Female].
----------------
{context}
Human: {question}
'''

prompt = PromptTemplate.from_template(prompt)
```

**Question-Answer Retrieval**

Finally, we will feed in all our information into our retrievalQA base. We will be using Llama2 as our base LLM, but there are options to use others. I chose this one because it is free and open-source. Note that in this instance, I am just searching the context to the 1 nearest node for similarity search. This can be modified based on how much context you would like to provide to your LLM.

```
ollama_model = 'llama3'

from langchain_community.chat_models.ollama import ChatOllama
from langchain.chains.retrieval_qa.base import RetrievalQA

vector_qa = RetrievalQA.from_chain_type(
    llm=ChatOllama(model=ollama_model), chain_type="stuff", retriever=vector_index.as_retriever(search_kwargs={'k': 1}),
    verbose=True, chain_type_kwargs={"verbose": True, "prompt": prompt}
)

pprint(vector_qa.invoke("How many persons with the name Maria do we have?"))
```

Our LLM responds:

```
>Finished chain.
{'query': 'How many persons with the name Maria do we have?',
'result': 'I can see that there are 2 entries in the context related to'
          'patients with the name Maria. One of them is of type *patient* and'
          'has the following last name: Rodriguez. So, there is one patient'
          'with the name Maria Rodriguez.'}
```

# Concluding Statements

Knowledge graphs in RAG with LLMs is poised for significant growth and widespread adoption due to its ability to combine external knowledge retrieval and semantic understanding of relationships, enabling systems to access and utilize up-to-date and accurate information. This makes them incredibly powerful for applications requiring real-time data and nuanced understanding. Additionally, KG with RAG opens up new possibilities for personalized and domain-specific applications. As industries increasingly seek intelligent and context-aware solutions, RAG’s seamless integration of large-scale data retrieval with advanced language generation will undoubtedly enhance its prominence in the future of AI.

# Resources and References

**All articles in my medium reading list:** [https://medium.com/@fatimaparada.taboada/list/rag-with-kg-research-006512fbe201](https://medium.com/@fatimaparada.taboada/list/rag-with-kg-research-006512fbe201)

**Project is heavily inspired by Sam Schifman’s RAG on FHIR data:** [https://github.com/samschifman/RAG\_on\_FHIR/tree/main/RAG\_on\_FHIR\_with\_KG](https://github.com/samschifman/RAG_on_FHIR/tree/main/RAG_on_FHIR_with_KG)

**Article by Anthony Alcaraz over KG versus Vector Search RAG:** https://ai.plainenglish.io/knowledge-graphs-achieve-superior-reasoning-versus-vector-search-forretrieval-augmentation-ec0b37b12c49

**Publication by Dr. Markus J Buehler on his findings with KG improving RAG:** [https://pubs.acs.org/doi/epdf/10.1021/acsengineeringau.3c00058](https://pubs.acs.org/doi/epdf/10.1021/acsengineeringau.3c00058)

**Dr. Jesus Barrasa’s video on semantic technologies with RAG:** https://www.youtube.com/watch?v=9DxwgIKVSHY&list=PL9Hl4pk2FsvX-5QPvwChBni\_mFF97rCE&index=6

**NebulaGraph’s article over the GraphRAG concept that uses Knowledge Graphs with LLMs and RAG**: https://medium.com/@nebulagraph/graph-rag-the-new-llm-stack-with-knowledgegraphs-e1e902c504e

![](https://miro.medium.com/v2/da:true/resize:fit:0/5c50caa54067fd622d2f0fac18392213bf92f6e2fae89b691e62bceb40885e74)

Medium Logo

Medium Logo

## Sign up to discover human stories that deepen your understanding of the world.

## Free

Distraction-free reading. No ads.

Organize your knowledge with lists and highlights.

Tell your story. Find your audience.

Sign up for free

## Membership

Read member-only stories

Support writers you read most

Earn money for your writing

Listen to audio narrations

Read offline with the Medium app

Try for $5/month

[Large Language Models](https://medium.com/tag/large-language-models?source=post_page-----4b12a114a20e---------------------------------------)

[Rdf](https://medium.com/tag/rdf?source=post_page-----4b12a114a20e---------------------------------------)

[Neo4j](https://medium.com/tag/neo4j?source=post_page-----4b12a114a20e---------------------------------------)

[Knowledge Graph](https://medium.com/tag/knowledge-graph?source=post_page-----4b12a114a20e---------------------------------------)

[Retrieval Augmented Gen](https://medium.com/tag/retrieval-augmented-gen?source=post_page-----4b12a114a20e---------------------------------------)

[![Fatima Parada-Taboada](https://miro.medium.com/v2/resize:fill:96:96/1*SxuU3Gp2DpXYtwf9ijjcbA.jpeg)](https://medium.com/@fatimaparada.taboada?source=post_page---post_author_info--4b12a114a20e---------------------------------------)

[![Fatima Parada-Taboada](https://miro.medium.com/v2/resize:fill:128:128/1*SxuU3Gp2DpXYtwf9ijjcbA.jpeg)](https://medium.com/@fatimaparada.taboada?source=post_page---post_author_info--4b12a114a20e---------------------------------------)

Follow

[**Written by Fatima Parada-Taboada**](https://medium.com/@fatimaparada.taboada?source=post_page---post_author_info--4b12a114a20e---------------------------------------)

[24 followers](https://medium.com/@fatimaparada.taboada/followers?source=post_page---post_author_info--4b12a114a20e---------------------------------------)

· [6 following](https://medium.com/@fatimaparada.taboada/following?source=post_page---post_author_info--4b12a114a20e---------------------------------------)

Software Engineer and M.S. Student for Artificial Intelligence . B.S. in Cognitive Science focus on Neuroscience and Computer Science. [linkedin.com/in/fatimapt](http://linkedin.com/in/fatimapt)

Follow

## Responses (1)

![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)

Write a response

[What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40fatimaparada.taboada%2Frag-on-csv-data-with-knowledge-graph-using-rdflib-rdflib-neo4j-and-langchain-4b12a114a20e&source=---post_responses--4b12a114a20e---------------------respond_sidebar------------------)

Cancel

Respond

[![Amit Sethi](https://miro.medium.com/v2/resize:fill:32:32/0*0dOeZkoJZ63fA7Iw)](https://medium.com/@amtisethi?source=post_page---post_responses--4b12a114a20e----0-----------------------------------)

[Amit Sethi](https://medium.com/@amtisethi?source=post_page---post_responses--4b12a114a20e----0-----------------------------------)

[Aug 4, 2024](https://medium.com/@amtisethi/thank-you-for-this-article-da360d068035?source=post_page---post_responses--4b12a114a20e----0-----------------------------------)

```

Thank you for this article. I have a question, why did you use the RDF library? NEO4J provides the mechanism to import the csv file. We can import the data and do the embedding of that data. I am trying to understand why we have to use RDF library…more
```

1 reply

Reply

## Recommended from Medium

[See more recommendations](https://medium.com/?source=post_page---read_next_recirc--4b12a114a20e---------------------------------------)

[Help](https://help.medium.com/hc/en-us?source=post_page-----4b12a114a20e---------------------------------------)

[Status](https://medium.statuspage.io/?source=post_page-----4b12a114a20e---------------------------------------)

[About](https://medium.com/about?autoplay=1&source=post_page-----4b12a114a20e---------------------------------------)

[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----4b12a114a20e---------------------------------------)

[Press](mailto:pressinquiries@medium.com)

[Blog](https://blog.medium.com/?source=post_page-----4b12a114a20e---------------------------------------)

[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4b12a114a20e---------------------------------------)

[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----4b12a114a20e---------------------------------------)

[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4b12a114a20e---------------------------------------)

[Text to speech](https://speechify.com/medium?source=post_page-----4b12a114a20e---------------------------------------)

[iframe](https://www.google.com/recaptcha/enterprise/anchor?ar=1&k=6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp&co=aHR0cHM6Ly9tZWRpdW0uY29tOjQ0Mw..&hl=en&v=jt8Oh2-Ue1u7nEbJQUIdocyd&size=invisible&cb=4ra6e2hxbdox)